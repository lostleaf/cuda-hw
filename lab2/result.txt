Output of command nvcc -gencode=arch=compute_30,code=\"sm_30,compute_30\" --ptxas-options=-v matrixmul_kernel.cu:

nvcc -gencode=arch=compute_30,code=\"sm_30,compute_30\" --pts info    : 0 bytes gmem
ptxas info    : Compiling entry function '_Z15MatrixMulKernel6MatrixS_S_' for 'sm_30'
ptxas info    : Function properties for _Z15MatrixMulKernel6MatrixS_S_
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 25 registers, 8448 bytes smem, 392 bytes cmem[0]
/usr/lib/gcc/x86_64-redhat-linux/4.4.7/../../../../lib64/crt1.o: In function `_start':
(.text+0x20): undefined reference to `main'
collect2: ld returned 1 exit statusxas-options=-v matrixmul_kernel.cu

So each thread use 25 registers and each block use 8448 bytes of shared memory.
Since our block size is 32 * 32, each block has 1024 threads.

warps per block = threads per block / threads per warp = 1024 / 32 = 32
block limit due to warps limit per SM= warp limit per SM / warps per block = 64 / 32 = 2

registers per warp = registers per thread * threads per warp = 25 * 32 = 800
Since register allocation unit size is 256, so each warp is allocated 1024 registers
warp limit due to registers per SM= registers per SM / registers allocated per warp = 65536 / 1024 = 64
block limit due to registers per SM= warp limit due to registers per SM / warps per block = 64 / 32 = 2

block limit due to shared memory = 49152 bytes / 8448 bytes = 5

So each SM can schedue 2 blocks

threads per SM = blocks per SM * threads per block = 2 * 1024 = 2048
threads in total = threads per SM * number of SM's = 2048 * 8 = 16384
